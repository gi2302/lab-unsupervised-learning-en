{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Import-and-Describe-the-Dataset\" data-toc-modified-id=\"Challenge-1---Import-and-Describe-the-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Import and Describe the Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?\" data-toc-modified-id=\"Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the dataset with mathematical and visualization techniques. What do you find?</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Data-Cleaning-and-Transformation\" data-toc-modified-id=\"Challenge-2---Data-Cleaning-and-Transformation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Data Cleaning and Transformation</a></span></li><li><span><a href=\"#Challenge-3---Data-Preprocessing\" data-toc-modified-id=\"Challenge-3---Data-Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Data Preprocessing</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.\" data-toc-modified-id=\"We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>We will use the <code>StandardScaler</code> from <code>sklearn.preprocessing</code> and scale our data. Read more about <code>StandardScaler</code> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" target=\"_blank\">here</a>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Data-Clustering-with-K-Means\" data-toc-modified-id=\"Challenge-4---Data-Clustering-with-K-Means-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Data Clustering with K-Means</a></span></li><li><span><a href=\"#Challenge-5---Data-Clustering-with-DBSCAN\" data-toc-modified-id=\"Challenge-5---Data-Clustering-with-DBSCAN-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Data Clustering with DBSCAN</a></span></li><li><span><a href=\"#Challenge-6---Compare-K-Means-with-DBSCAN\" data-toc-modified-id=\"Challenge-6---Compare-K-Means-with-DBSCAN-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Compare K-Means with DBSCAN</a></span></li><li><span><a href=\"#Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters\" data-toc-modified-id=\"Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge 2 - Changing K-Means Number of Clusters</a></span></li><li><span><a href=\"#Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples\" data-toc-modified-id=\"Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Bonus Challenge 3 - Changing DBSCAN <code>eps</code> and <code>min_samples</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures that the plots appear within the notebook, not in a separate window.\n",
    "%matplotlib inline  \n",
    "\n",
    "# Importing libraries\n",
    "import matplotlib.pyplot as plt  # For plotting graphs and visualizations\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and reading datasets\n",
    "import seaborn as sns  # For advanced statistical visualizations\n",
    "import warnings  # For controlling warning messages\n",
    "from sklearn.exceptions import DataConversionWarning  # To specifically catch data conversion warnings\n",
    "\n",
    "# Suppress warnings that are not relevant to the analysis to keep the output clean\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Import and Describe the Dataset\n",
    "\n",
    "In this lab, we will use a dataset containing information about customer preferences. We will look at how much each customer spends in a year on each subcategory in the grocery store and try to find similarities using clustering.\n",
    "\n",
    "The origin of the dataset is [here](https://archive.ics.uci.edu/ml/datasets/wholesale+customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data: Wholesale customers data\n",
    "\n",
    "# Reading the dataset into a DataFrame with a relative file path\n",
    "customers = pd.read_csv('../data/Wholesale customers data.csv')\n",
    "\n",
    "# Displaying the first few rows of the dataset\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset with mathematical and visualization techniques. What do you find?\n",
    "\n",
    "Checklist:\n",
    "\n",
    "* What does each column mean?\n",
    "* Any categorical data to convert?\n",
    "* Any missing data to remove?\n",
    "* Column collinearity - any high correlations?\n",
    "* Descriptive statistics - any outliers to remove?\n",
    "* Column-wise data distribution - is the distribution skewed?\n",
    "* Etc.\n",
    "\n",
    "Additional info: Over a century ago, an Italian economist named Vilfredo Pareto discovered that roughly 20% of the customers account for 80% of the typical retail sales. This is called the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle). Check if this dataset displays this characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the dataset with mathematical and visualization techniques\n",
    "\n",
    "# 1. What is the shape of the dataframe (rows, columns)?\n",
    "# Displaying the shape of the dataframe (rows, columns)\n",
    "print(\"\\nShape of the DataFrame:\")\n",
    "print(customers.shape)\n",
    "\n",
    "# 2. Any missing data to remove?\n",
    "# Checking for missing data in the dataset\n",
    "print(\"\\nMissing Data:\")\n",
    "print(customers.isnull().sum())\n",
    "\n",
    "# 3. Duplicate row check\n",
    "# Checking if there are any duplicate rows in the dataset\n",
    "print(\"\\nChecking for Duplicate Rows:\")\n",
    "print(customers.duplicated().sum())\n",
    "\n",
    "# 4. What does each column mean?\n",
    "# Displaying the column names and a brief explanation (if known).\n",
    "print(\"\\nColumn Names and Descriptions:\")\n",
    "print(customers.columns)\n",
    "\n",
    "# 5. Any categorical data to convert?\n",
    "# Checking the data types and identifying categorical columns\n",
    "print(\"\\nData Types (Check for categorical columns):\")\n",
    "print(customers.dtypes)\n",
    "\n",
    "# 6. What are the unique values in each column? Are any of the columns categorical?\n",
    "# Displaying the unique values of each column to help decide if they are categorical or quantitative\n",
    "print(\"\\nUnique Values for Each Column:\")\n",
    "for column in customers.columns:\n",
    "    print(f\"{column}: {customers[column].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "### Your observations here\n",
    "\n",
    "- **Shape of the DataFrame**:\n",
    "  - The dataset contains **440 rows** and **8 columns**, meaning it has data for 440 customers and 8 different attributes (features).\n",
    "\n",
    "- **Missing Data**:\n",
    "  - There are **no missing values** in any of the columns. All 440 rows have complete data for each feature, which is a positive sign for data integrity.\n",
    "\n",
    "- **Checking for Duplicate Rows**:\n",
    "  - There are **no duplicate rows** in the dataset, ensuring that each customer record is unique.\n",
    "\n",
    "- **Column Names and Descriptions**:\n",
    "  - The dataset consists of the following columns:\n",
    "    - `Channel`: Likely represents different sales channels (categorical).\n",
    "    - `Region`: Likely represents geographic regions (categorical).\n",
    "    - `Fresh`: Spending on fresh products (quantitative).\n",
    "    - `Milk`: Spending on milk (quantitative).\n",
    "    - `Grocery`: Spending on grocery products (quantitative).\n",
    "    - `Frozen`: Spending on frozen products (quantitative).\n",
    "    - `Detergents_Paper`: Spending on detergents and paper products (quantitative).\n",
    "    - `Delicassen`: Spending on delicatessen products (quantitative).\n",
    "\n",
    "- **Data Types**:\n",
    "  - The data types of the columns are as follows:\n",
    "    - **Quantitative** (spending values): `Fresh`, `Milk`, `Grocery`, `Frozen`, `Detergents_Paper`, `Delicassen`\n",
    "    - **Categorical** (representing categories or groups): `Channel`, `Region`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Categorical Variables: Channel and Region\n",
    "\n",
    "# 1. Frequency counts for the categorical variables\n",
    "print(\"\\nFrequency Counts for Categorical Variables:\")\n",
    "print(customers['Channel'].value_counts())\n",
    "print(customers['Region'].value_counts())\n",
    "\n",
    "# 2. Bar plots to visualize the distribution of the categorical variables\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='Channel', data=customers)\n",
    "plt.title('Distribution of Channel')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='Region', data=customers)\n",
    "plt.title('Distribution of Region')\n",
    "plt.show()\n",
    "\n",
    "# 3. Cross-tabulation of Channel and Region (if applicable)\n",
    "print(\"\\nCross-tabulation of Channel and Region:\")\n",
    "print(pd.crosstab(customers['Channel'], customers['Region']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "### 1. Frequency Tables:\n",
    "- We have seen the frequency distribution of the `Channel` and `Region` variables:\n",
    "  - **Channel** has two values: `1` and `2`. The majority of customers are in `Channel 1`.\n",
    "  - **Region** has three values: `1`, `2`, and `3`. Most customers are from `Region 3`, followed by `Region 1`, and very few from `Region 2`.\n",
    "\n",
    "### 2. Distribution of Channel:\n",
    "- A **bar plot** has shown that **Channel 1** has a significantly higher count of customers than **Channel 2**. This suggests that **Channel 1** is more commonly used by the customers in this dataset.\n",
    "\n",
    "### 3. Distribution of Region:\n",
    "- The **bar plot** of `Region` indicates that the vast majority of customers belong to **Region 3**, while **Region 1** has a smaller proportion of customers, and **Region 2** has even fewer customers.\n",
    "\n",
    "### 4. Cross-tabulation of Channel and Region:\n",
    "- The **cross-tabulation** between `Channel` and `Region` gives us a more detailed view of the distribution of customers across both variables. Here's a breakdown:\n",
    "  - **For Channel 1**:\n",
    "    - 59 customers belong to **Region 1**.\n",
    "    - 28 customers belong to **Region 2**.\n",
    "    - 211 customers belong to **Region 3**.\n",
    "  - **For Channel 2**:\n",
    "    - 18 customers belong to **Region 1**.\n",
    "    - 19 customers belong to **Region 2**.\n",
    "    - 105 customers belong to **Region 3**.\n",
    "  \n",
    "  This suggests that **Channel 1** is the most common channel across all regions, especially in **Region 3**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Quantitative Variables\n",
    "\n",
    "# Selecting the quantitative columns\n",
    "quantitative_columns = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']\n",
    "\n",
    "# 1. Descriptive statistics - including mean, standard deviation, min, 25%, 50%, 75%, and max\n",
    "print(\"\\nDescriptive Statistics for Quantitative Variables:\")\n",
    "print(customers[quantitative_columns].describe())\n",
    "\n",
    "# 2. Distribution of each quantitative variable (using histograms)\n",
    "print(\"\\nDistribution of Quantitative Variables:\")\n",
    "customers[quantitative_columns].hist(figsize=(12, 10), bins=20)\n",
    "plt.suptitle('Histograms of Quantitative Columns')\n",
    "plt.show()\n",
    "\n",
    "# 3. Boxplots for each quantitative variable (to check for outliers)\n",
    "print(\"\\nBoxplots of Quantitative Variables:\")\n",
    "customers[quantitative_columns].plot(kind='box', figsize=(12, 8), vert=False)\n",
    "plt.suptitle('Boxplots of Quantitative Columns')\n",
    "plt.show()\n",
    "\n",
    "# 4. Outlier detection using IQR method (based on the 25th and 75th percentiles)\n",
    "Q1 = customers[quantitative_columns].quantile(0.25)\n",
    "Q3 = customers[quantitative_columns].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = ((customers[quantitative_columns] < (Q1 - 1.5 * IQR)) | (customers[quantitative_columns] > (Q3 + 1.5 * IQR)))\n",
    "print(\"\\nOutliers (using IQR method):\")\n",
    "print(outliers_iqr.sum())  # Shows how many outliers exist for each column\n",
    "\n",
    "# 5. Outlier detection using z-score method\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Calculating z-scores for each column\n",
    "z_scores = customers[quantitative_columns].apply(zscore)\n",
    "outliers_zscore = (z_scores > 3) | (z_scores < -3)  # Consider outliers with z-score > 3 or < -3\n",
    "print(\"\\nOutliers (using Z-score method):\")\n",
    "print(outliers_zscore.sum())  # Shows how many outliers exist for each column\n",
    "\n",
    "# 6. Skewness for each quantitative variable\n",
    "print(\"\\nSkewness of Each Quantitative Variable:\")\n",
    "skewness = customers[quantitative_columns].skew()\n",
    "print(skewness)\n",
    "\n",
    "# 7. Pareto principle - Top 20% contribution to total sales for each category\n",
    "print(\"\\nTop 20% Cumulative Contribution to Total Sales (Pareto Principle):\")\n",
    "# Calculate the total spending for each category\n",
    "total_sales = customers[quantitative_columns].sum(axis=0).sort_values(ascending=False)\n",
    "cumulative_sales = total_sales.cumsum() / total_sales.sum()\n",
    "print(cumulative_sales)\n",
    "\n",
    "# Visualizing the Pareto principle for the top categories\n",
    "cumulative_sales.plot(kind='line', marker='o', figsize=(10, 6))\n",
    "plt.title('Cumulative Sum of Sales Categories (Pareto Principle)')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Cumulative Percentage of Total Sales')\n",
    "plt.show()\n",
    "\n",
    "# 8. Correlation matrix for quantitative variables\n",
    "print(\"\\nCorrelation Matrix for Quantitative Variables:\")\n",
    "correlation_matrix = customers[quantitative_columns].corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Plotting the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Quantitative Variables')\n",
    "plt.show()\n",
    "\n",
    "# 9. Pairwise plots for the quantitative variables to visualize relationships\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\nPairwise Plots of Quantitative Variables:\")\n",
    "sns.pairplot(customers[quantitative_columns])\n",
    "plt.suptitle('Pairwise Plots of Quantitative Variables', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your observations here\n",
    "\n",
    "- **Descriptive Statistics for Quantitative Variables**:\n",
    "  The summary statistics for the quantitative variables are as follows:\n",
    "  - `Fresh`: Mean = 12000.30, Std = 12647.33, Min = 3, Max = 112151\n",
    "  - `Milk`: Mean = 5796.27, Std = 7380.38, Min = 55, Max = 73498\n",
    "  - `Grocery`: Mean = 7951.28, Std = 9503.16, Min = 3, Max = 92780\n",
    "  - `Frozen`: Mean = 3071.93, Std = 4854.67, Min = 25, Max = 60869\n",
    "  - `Detergents_Paper`: Mean = 2881.49, Std = 4767.85, Min = 3, Max = 40827\n",
    "  - `Delicassen`: Mean = 1524.87, Std = 2820.11, Min = 3, Max = 47943\n",
    "\n",
    "- **Distribution of Quantitative Variables**:\n",
    "  The histograms show the distribution of each quantitative variable. Most variables show a skewed distribution.\n",
    "\n",
    "- **Boxplots of Quantitative Variables**:\n",
    "  The boxplots display the spread of the data for each quantitative variable. All variables have outliers, indicated by the points outside the whiskers of the boxplot. Variables like `Fresh`, `Milk`, and `Grocery` have significant outliers.\n",
    "\n",
    "- **Outliers**:\n",
    "  - Using the IQR method, the following outlier counts are observed for each quantitative variable:\n",
    "    - `Fresh`: 20\n",
    "    - `Milk`: 28\n",
    "    - `Grocery`: 24\n",
    "    - `Frozen`: 43\n",
    "    - `Detergents_Paper`: 30\n",
    "    - `Delicassen`: 27\n",
    "\n",
    "  - Using the Z-score method, the following outlier counts are observed:\n",
    "    - `Fresh`: 7\n",
    "    - `Milk`: 9\n",
    "    - `Grocery`: 7\n",
    "    - `Frozen`: 6\n",
    "    - `Detergents_Paper`: 10\n",
    "    - `Delicassen`: 4\n",
    "\n",
    "  These outlier counts indicate that there are several extreme values in these features, which may require further attention or removal based on the analysis.\n",
    "\n",
    "- **Skewness of Each Quantitative Variable**:\n",
    "  The skewness values for the quantitative variables indicate a right skew in most variables:\n",
    "  - `Fresh`: 2.56 (highly skewed)\n",
    "  - `Milk`: 4.05 (highly skewed)\n",
    "  - `Grocery`: 3.59 (highly skewed)\n",
    "  - `Frozen`: 5.91 (highly skewed)\n",
    "  - `Detergents_Paper`: 3.63 (highly skewed)\n",
    "  - `Delicassen`: 11.15 (extremely skewed)\n",
    "\n",
    "  Variables with skewness greater than 1 indicate significant right skewness, suggesting the need for potential transformations or further analysis.\n",
    "\n",
    "- **Pareto Principle (Top 20% Cumulative Contribution to Total Sales)**:\n",
    "  The Pareto Principle highlights that roughly 20% of the categories contribute to 80% of the total sales. In this case, the cumulative contribution of the top categories is observed as follows:\n",
    "  - The first 20% of categories such as *Fresh*, *Grocery*, and *Milk* contribute the majority of the total sales.\n",
    "  - *Delicassen* represents the remaining part of the total sales.\n",
    "\n",
    "- **Correlation Matrix for Quantitative Variables**:\n",
    "  The correlation matrix for the quantitative variables indicates the following:\n",
    "  - `Fresh` has a moderate positive correlation with `Frozen` (0.35) and a low positive correlation with `Delicassen` (0.24).\n",
    "  - `Milk` has a high positive correlation with `Grocery` (0.73) and `Detergents_Paper` (0.66), suggesting that these variables tend to increase together.\n",
    "  - `Grocery` has a very high positive correlation with `Detergents_Paper` (0.92), indicating that spending in these categories is closely related.\n",
    "  - `Frozen` has a low negative correlation with `Detergents_Paper` (-0.13), showing a weak inverse relationship.\n",
    "  - `Delicassen` has moderate positive correlations with `Milk` (0.41) and `Frozen` (0.39).\n",
    "\n",
    "- **Correlation Matrix Plot**:\n",
    "  The correlation matrix heatmap visually represents the strength of relationships between the quantitative variables. As seen in the plot, strong correlations are highlighted in red, while weaker correlations are shown in blue.\n",
    "\n",
    "- **Pairwise Plots of Quantitative Variables**:\n",
    "  The pairwise plots show the scatterplots and histograms for each pair of quantitative variables. These plots visually demonstrate the relationships between each pair of features, with clear signs of skewness in most of the quantitative variables. The diagonal histograms show the individual distribution of each variable. Additionally, the scatter plots suggest some potential relationships, such as the strong association between `Milk` and `Grocery`, which is also reflected in the correlation matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Data Cleaning and Transformation\n",
    "\n",
    "If your conclusion from the previous challenge is the data need cleaning/transformation, do it in the cells below. However, if your conclusion is the data need not be cleaned or transformed, feel free to skip this challenge. But if you do choose the latter, please provide rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Challenge 2 - Data Cleaning and Transformation\n",
    "# From the previous analysis (Challenge 1), we noticed that:\n",
    "# 1. There are outliers in several quantitative columns (e.g., Fresh, Milk, Grocery, etc.).\n",
    "# 2. Some variables like Fresh, Milk, Grocery, and others are highly skewed.\n",
    "# 3. There are no missing values or duplicate rows in the dataset.\n",
    "# Based on these insights, we may consider transforming skewed variables and/or handling outliers.\n",
    "# Below, we will:\n",
    "# - Apply transformations if necessary (e.g., log transformation for skewed variables)\n",
    "# - Handle outliers using different strategies (e.g., removal, capping)\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Log transformation for highly skewed columns (if necessary)\n",
    "skewed_columns = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']\n",
    "customers_transformed = customers.copy()\n",
    "\n",
    "# Apply log transformation to reduce skewness\n",
    "for col in skewed_columns:\n",
    "    customers_transformed[col] = np.log(customers_transformed[col] + 1)\n",
    "\n",
    "# Visualize the distribution after transformation\n",
    "print(\"\\nDistribution of Quantitative Variables after Transformation:\")\n",
    "customers_transformed[skewed_columns].hist(figsize=(12, 10), bins=20)\n",
    "plt.suptitle('Histograms of Quantitative Columns (After Transformation)')\n",
    "plt.show()\n",
    "\n",
    "# Handling outliers (IQR method)\n",
    "Q1 = customers_transformed[skewed_columns].quantile(0.25)\n",
    "Q3 = customers_transformed[skewed_columns].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define a threshold for identifying outliers\n",
    "outlier_threshold = 1.5\n",
    "\n",
    "# Remove outliers based on IQR method (capping or removal could be an option here as well)\n",
    "customers_no_outliers = customers_transformed[~((customers_transformed[skewed_columns] < (Q1 - outlier_threshold * IQR)) | \n",
    "                                                (customers_transformed[skewed_columns] > (Q3 + outlier_threshold * IQR))).any(axis=1)]\n",
    "\n",
    "# Checking the shape of the dataset after transformations\n",
    "print(f\"\\nShape after transformation and outlier removal: {customers_no_outliers.shape}\")\n",
    "\n",
    "# Leave space for better readability\n",
    "print(\"\\n\")  # Adding space after shape print\n",
    "\n",
    "# Display a few rows to verify the changes\n",
    "customers_no_outliers.head()\n",
    "\n",
    "# Checking the descriptive statistics again after the transformation\n",
    "print(\"\\nDescriptive Statistics for Transformed Quantitative Variables:\")\n",
    "print(customers_no_outliers[skewed_columns].describe())\n",
    "\n",
    "# Rechecking skewness after transformation\n",
    "print(\"\\nSkewness of Transformed Quantitative Variables:\")\n",
    "skewness_after_transformation = customers_no_outliers[skewed_columns].skew()\n",
    "print(skewness_after_transformation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your observations here\n",
    "\n",
    "- **Descriptive Statistics for Transformed Quantitative Variables**:\n",
    "  The summary statistics for the transformed quantitative variables are as follows:\n",
    "  - `Fresh`: Mean = 8.93, Std = 1.12, Min = 5.55, Max = 11.63\n",
    "  - `Milk`: Mean = 8.12, Std = 1.01, Min = 5.31, Max = 10.90\n",
    "  - `Grocery`: Mean = 8.42, Std = 1.01, Min = 5.41, Max = 11.44\n",
    "  - `Frozen`: Mean = 7.43, Std = 1.13, Min = 4.52, Max = 10.46\n",
    "  - `Detergents_Paper`: Mean = 6.79, Std = 1.61, Min = 1.79, Max = 10.62\n",
    "  - `Delicassen`: Mean = 6.80, Std = 1.03, Min = 3.85, Max = 9.71\n",
    "\n",
    "- **Distribution of Quantitative Variables after Transformation**:\n",
    "  The histograms show the distribution of each transformed quantitative variable. After applying the log transformation, most variables exhibit more symmetric and less skewed distributions, with the exception of `Frozen`, `Milk`, and `Delicassen` which still display some mild skewness.\n",
    "\n",
    "- **Skewness of Transformed Quantitative Variables**:\n",
    "  The skewness values for the transformed quantitative variables indicate that the transformations were successful in reducing skewness:\n",
    "  - `Fresh`: -0.68 (moderately skewed to the left)\n",
    "  - `Milk`: -0.04 (approximately normal)\n",
    "  - `Grocery`: 0.03 (approximately normal)\n",
    "  - `Frozen`: -0.13 (slightly skewed to the left)\n",
    "  - `Detergents_Paper`: -0.02 (approximately normal)\n",
    "  - `Delicassen`: -0.35 (slightly skewed to the left)\n",
    "\n",
    "  These skewness values suggest that the transformation has made the data more normally distributed, which will be beneficial for clustering or other machine learning tasks.\n",
    "\n",
    "- **Shape After Transformation and Outlier Removal**:\n",
    "  The dataset now has 398 rows and 8 columns after the transformation and outlier removal, reducing the size of the dataset slightly by removing outliers.\n",
    "\n",
    "---\n",
    "\n",
    "The transformation and outlier removal steps seem to have made significant improvements to the data's distribution and shape, setting the dataset up for further analysis or machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Data Preprocessing\n",
    "\n",
    "One problem with the dataset is the value ranges are remarkably different across various categories (e.g. `Fresh` and `Grocery` compared to `Detergents_Paper` and `Delicassen`). If you made this observation in the first challenge, you've done a great job! This means you not only completed the bonus questions in the previous Supervised Learning lab but also researched deep into [*feature scaling*](https://en.wikipedia.org/wiki/Feature_scaling). Keep on the good work!\n",
    "\n",
    "Diverse value ranges in different features could cause issues in our clustering. The way to reduce the problem is through feature scaling. We'll use this technique again with this dataset.\n",
    "\n",
    "#### We will use the `StandardScaler` from `sklearn.preprocessing` and scale our data. Read more about `StandardScaler` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "*After scaling your data, assign the transformed data to a new variable `customers_scale`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your import here:\n",
    "\n",
    "# Importing the StandardScaler class from sklearn.preprocessing\n",
    "# StandardScaler is used to standardize the features by removing the mean and scaling to unit variance.\n",
    "# This is especially useful for machine learning models that are sensitive to the scale of the data, such as clustering algorithms.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "# Applying StandardScaler to scale the data\n",
    "\n",
    "# Select the columns to scale (quantitative variables)\n",
    "quantitative_columns = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "# The StandardScaler will scale the data so that each feature will have a mean of 0 and a standard deviation of 1.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data in one step\n",
    "# The fit method computes the mean and standard deviation for each feature,\n",
    "# and the transform method scales the data accordingly.\n",
    "customers_scale = customers_no_outliers[quantitative_columns].copy()\n",
    "customers_scale[quantitative_columns] = scaler.fit_transform(customers_scale[quantitative_columns])\n",
    "\n",
    "# Checking the shape of the scaled data\n",
    "# This ensures that the number of rows and columns has not changed after scaling.\n",
    "print(f\"Shape after scaling: {customers_scale.shape}\")\n",
    "\n",
    "# Leave space for better readability\n",
    "print(\"\\n\")  # Adding space after shape print\n",
    "\n",
    "# Displaying the first few rows to verify the scaling\n",
    "# This will show the scaled values for a quick check.\n",
    "customers_scale.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 - Data Preprocessing: Feature Scaling\n",
    "\n",
    "In this challenge, we addressed the issue of different value ranges across various categories (e.g., `Fresh` and `Grocery` compared to `Detergents_Paper` and `Delicassen`). This discrepancy can impact models like clustering, where the scale of features can significantly affect the results.\n",
    "\n",
    "#### Why Scale Features?\n",
    "Scaling helps bring all the features to the same scale, ensuring that each feature contributes equally to the model's performance. For example, without scaling, variables with larger ranges like `Fresh` and `Grocery` could dominate the model, while features like `Detergents_Paper` and `Delicassen` might not be given enough importance.\n",
    "\n",
    "#### Standardization: The Approach We Used\n",
    "We used **StandardScaler** from `sklearn.preprocessing` to standardize the data. Standardization scales the data by removing the mean and scaling to unit variance. This transforms the data so that it has a mean of 0 and a standard deviation of 1, which is especially important for machine learning algorithms that are sensitive to the scale of data.\n",
    "\n",
    "- **Formula for Standardization:**\n",
    "  $$\n",
    "  z = \\frac{(X - \\mu)}{\\sigma}\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $X$ is the value of the feature,\n",
    "  - $\\mu$ is the mean of the feature,\n",
    "  - $\\sigma$ is the standard deviation of the feature.\n",
    "\n",
    "#### Effect of Scaling on the Data\n",
    "After applying the **StandardScaler**, we can see that the range of each feature is now standardized, making it easier for machine learning models to interpret and process the data effectively. The output shows that each feature now has a mean close to 0 and a standard deviation of 1.\n",
    "\n",
    "#### Shape and Transformation\n",
    "We checked the shape of the dataset after scaling, which remains consistent at **398 rows** and **6 columns** (for the quantitative variables). The scaled data is now ready for use in modeling tasks that require normalized data.\n",
    "\n",
    "We also printed the first few rows of the scaled data to verify the transformations.\n",
    "\n",
    "This transformation ensures that the models will treat all features equally, without any one feature dominating the learning process due to scale differences.\n",
    "\n",
    "Next steps will involve applying clustering or other machine learning models, depending on your goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition from Feature Scaling to Data Quality Check\n",
    "\n",
    "After completing the feature scaling in **Challenge 3**, where we used **StandardScaler** to standardize the data, the dataset is now ready for modeling. However, before we proceed with clustering or any further analysis, it is important to ensure that the scaled data is clean and free from any issues that might affect the results.\n",
    "\n",
    "#### Why Check for Missing or Erroneous Values?\n",
    "While we performed the necessary transformations in **Challenge 3**, it's always a good practice to verify that the scaling process hasn't introduced any issues such as missing values or unexpected values (like negative values). These issues can distort the clustering results, so we will perform a final check.\n",
    "\n",
    "#### What We Will Do:\n",
    "- **Check for missing values**: We will verify if there are any missing values in the scaled dataset.\n",
    "- **Check for negative values**: After the transformation, values should be non-negative. Any negative values could indicate issues during the log transformation or scaling.\n",
    "- **Check for extreme values**: We will inspect the minimum and maximum values to ensure they fall within a reasonable range.\n",
    "\n",
    "We will also visualize the relationships between the scaled features to confirm that they look consistent after scaling. This will help us understand the structure of the data before we move on to clustering.\n",
    "\n",
    "Let's now perform these checks to ensure the data is clean and ready for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values in the scaled data\n",
    "print(\"\\nChecking for Missing Values in Scaled Data:\")\n",
    "print(customers_scale.isnull().sum())\n",
    "\n",
    "# Checking for any erroneous values (e.g., negative values after log transformation or scaling)\n",
    "# Since we applied log transformation and scaling, the values should be non-negative and within a reasonable range\n",
    "# We can check for any negative values in the scaled data (which might indicate issues in transformation)\n",
    "print(\"\\nChecking for Negative Values in Scaled Data:\")\n",
    "print((customers_scale < 0).sum())\n",
    "\n",
    "# Checking for any other anomalies, such as extremely large or small values\n",
    "# You can define thresholds if needed, but let's just inspect the max and min values\n",
    "print(\"\\nMaximum and Minimum Values in Scaled Data:\")\n",
    "print(customers_scale.min())\n",
    "print(customers_scale.max())\n",
    "\n",
    "# If necessary, you can also visualize the scaled data using pairplots to inspect the relationships visually\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.pairplot(customers_scale)\n",
    "plt.suptitle('Pairwise Plots of Scaled Quantitative Variables', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Visualizing the Data\n",
    "\n",
    "In this section, we reviewed the scaled data after applying the **StandardScaler** from **Challenge 3**. The goal of scaling was to standardize the data, ensuring that each feature has a mean of 0 and a standard deviation of 1. This transformation is crucial for many machine learning algorithms, like K-Means clustering, that are sensitive to the scale of the data.\n",
    "\n",
    "#### 1. **Checking for Missing and Negative Values**\n",
    "We first checked for any missing values in the scaled dataset. As expected, there were no missing values (all counts were 0). Then, we examined whether any values in the scaled dataset were negative:\n",
    "- **Negative values**: This is typical in scaled data since scaling shifts the values based on the mean of each feature. For example, features like `Fresh`, `Milk`, and `Grocery` have negative values, indicating that the data points are below the feature mean, which is expected behavior after scaling.\n",
    "\n",
    "#### 2. **Range of Values in the Scaled Data**\n",
    "Next, we checked the minimum and maximum values for each feature:\n",
    "- The range of values in the scaled data shows that the features have been standardized, with negative values for minimums and positive values for maximums. This confirms that the scaling worked as intended, and the data is now on a comparable scale.\n",
    "\n",
    "#### 3. **Pairwise Plots of Scaled Quantitative Variables**\n",
    "To visualize the relationships between the scaled features, we created **pairwise plots**:\n",
    "- The **diagonal histograms** show the distribution of each scaled feature.\n",
    "- The **scatter plots** off the diagonal display the relationships between pairs of features. Some pairs, like `Milk` and `Grocery`, show clear linear relationships, which corresponds with their high correlation in the correlation matrix.\n",
    "\n",
    "These steps and visualizations confirm that the data has been successfully scaled and is now ready for clustering or other analyses.\n",
    "\n",
    "- **Next steps**: We will proceed to clustering the data in **Challenge 4** by applying K-Means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Multicollinearity in the Scaled Data\n",
    "\n",
    "After scaling the data in **Challenge 3**, it's crucial to assess whether any multicollinearity exists among the features. Multicollinearity can undermine the performance of clustering algorithms, such as K-Means, because it can make it harder for the model to distinguish the unique contribution of each feature. This is especially important if we decide to use algorithms sensitive to feature correlations.\n",
    "\n",
    "#### 1. **Correlation Matrix of Scaled Quantitative Variables**\n",
    "We will compute the **correlation matrix** to check how strongly each feature is correlated with others. High correlations between features can indicate potential multicollinearity, which might require us to remove one of the correlated features (like **Milk** and **Grocery**, which had a high correlation earlier).\n",
    "\n",
    "#### 2. **Variance Inflation Factor (VIF)**\n",
    "Additionally, we'll calculate the **Variance Inflation Factor (VIF)** for each feature. VIF quantifies how much the variance of a regression coefficient is inflated due to collinearity with other features. A VIF greater than 5 or 10 may indicate problematic multicollinearity.\n",
    "\n",
    "Let's now proceed with computing the correlation matrix and VIF to evaluate the multicollinearity in the scaled dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Step 1: Compute the correlation matrix\n",
    "correlation_matrix = customers_scale.corr()\n",
    "\n",
    "# Print the correlation matrix of the scaled quantitative variables\n",
    "print(\"\\nCorrelation Matrix of Scaled Quantitative Variables:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Step 2: Plot the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Scaled Quantitative Variables')\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Calculate Variance Inflation Factor (VIF)\n",
    "X = add_constant(customers_scale)  # Adding constant for intercept\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(\"\\nVariance Inflation Factor (VIF) for each feature:\")\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Multicollinearity in the Scaled Data\n",
    "\n",
    "In this section, we assessed the correlation between the features in the scaled data to understand how correlated the variables are and whether there might be multicollinearity issues that could affect the clustering algorithms (K-Means and DBSCAN).\n",
    "\n",
    "#### 1. **Correlation Matrix of Scaled Quantitative Variables**:\n",
    "The correlation matrix was computed to analyze the relationships between the scaled quantitative features. The matrix shows how strongly each feature is correlated with others. Here are some key observations:\n",
    "- **Milk** and **Grocery** have a high positive correlation (0.78), indicating that when one increases, the other tends to increase as well.\n",
    "- **Detergents_Paper** and **Grocery** also show a strong positive correlation (0.79), suggesting they move together.\n",
    "- There are no features with perfect correlation (1), but some moderate correlations exist, particularly between related categories like **Frozen** and **Fresh**.\n",
    "\n",
    "#### 2. **Correlation Matrix Plot**:\n",
    "The plot visually confirms the correlation results, where stronger correlations are shown in red and weaker correlations in blue. As expected, **Milk** and **Grocery** have a significant red block, indicating a strong positive relationship.\n",
    "\n",
    "#### 3. **Variance Inflation Factor (VIF)**:\n",
    "VIF measures the multicollinearity of each feature by quantifying how much a feature is correlated with other features. High VIF values indicate high multicollinearity, which can be problematic for models like K-Means that are sensitive to feature scaling.\n",
    "- **VIF values** below 5 generally indicate low multicollinearity. In this case, all features have relatively low VIFs:\n",
    "  - The highest VIF is for **Grocery** (3.99), which indicates some degree of multicollinearity with other features, particularly **Milk** and **Detergents_Paper**.\n",
    "  - The VIF values for **Fresh**, **Frozen**, **Delicassen**, and **Detergents_Paper** are low, indicating they are not highly collinear with other features.\n",
    "\n",
    "Given that the VIFs are generally low, no further action is needed regarding multicollinearity before proceeding with clustering. However, the strong correlation between **Milk** and **Grocery** suggests that these features might share some common information, and in clustering, removing one might make sense to reduce redundancy.\n",
    "\n",
    "These checks provide confidence that the data is well-prepared for clustering, and no further transformations are necessary before proceeding to Challenge 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Clustering with PCA\n",
    "\n",
    "After addressing multicollinearity, we can proceed with **exploratory clustering** to visualize how the data behaves in lower dimensions. As we observed, **Milk** and **Grocery** are highly correlated (0.78), and **Grocery** and **Detergents_Paper** also show a strong correlation (0.79), suggesting that these features might share similar information. This redundancy could affect clustering performance, so exploring these relationships using **Principal Component Analysis (PCA)** can help us understand how the features interact and contribute to the clustering process.\n",
    "\n",
    "#### 1. **Why PCA?**\n",
    "PCA helps reduce the dimensionality of the data by transforming the original features into a smaller set of uncorrelated components (principal components) that capture most of the variance. By visualizing the data in 2D or 3D, we can assess whether natural clusters emerge and gain insights into how features like **Milk**, **Grocery**, **Detergents_Paper**, and other variables behave together.\n",
    "\n",
    "#### 2. **Applying PCA**:\n",
    "We will apply PCA to the scaled data and reduce it to 2 dimensions to visualize the distribution and possible clustering of the data. This allows us to explore whether the highly correlated features (e.g., **Milk**, **Grocery**, and **Detergents_Paper**) contribute similarly to the clustering.\n",
    "\n",
    "#### 3. **Next Steps**:\n",
    "- We will apply **PCA** to the scaled dataset.\n",
    "- Visualize the results in 2D and examine whether distinct groups or patterns emerge in the data.\n",
    "- This will give us further insights before applying more complex clustering algorithms like **K-Means** and **DBSCAN** in **Challenge 4**.\n",
    "\n",
    "Let's proceed with applying **PCA** and visualizing the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Apply PCA to reduce the data to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(customers_scale)\n",
    "\n",
    "# Step 2: Create a DataFrame to hold the PCA components\n",
    "pca_df = pd.DataFrame(data=pca_components, columns=['PCA1', 'PCA2'])\n",
    "\n",
    "# Step 3: Visualize the PCA results in a 2D scatter plot (all features considered)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_df['PCA1'], pca_df['PCA2'], c='blue', edgecolors='k', alpha=0.7)\n",
    "plt.title('PCA of Scaled Data - 2D Visualization', fontsize=14)\n",
    "plt.xlabel('PCA1', fontsize=12)\n",
    "plt.ylabel('PCA2', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Explained variance to understand how much information each principal component explains\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"\\nExplained Variance Ratio for the 2 Principal Components:\")\n",
    "print(explained_variance)\n",
    "\n",
    "# Step 5: Analyze the component loadings (for all features)\n",
    "loadings = pca.components_  # The loadings give us the weight of each feature in the components\n",
    "features = customers_scale.columns\n",
    "\n",
    "# Create a DataFrame of the loadings for better visualization\n",
    "loadings_df = pd.DataFrame(loadings.T, columns=['PC1', 'PC2'], index=features)\n",
    "print(\"\\nComponent Loadings:\")\n",
    "print(loadings_df)\n",
    "\n",
    "# Visualizing the loadings for each feature\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(loadings_df.index, loadings_df['PC1'], alpha=0.6, label='PC1')\n",
    "plt.bar(loadings_df.index, loadings_df['PC2'], alpha=0.6, label='PC2')\n",
    "plt.title('PCA Component Loadings for Scaled Features', fontsize=14)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Loading Value', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Clustering with PCA\n",
    "\n",
    "In this section, we applied **Principal Component Analysis (PCA)** to reduce the dimensionality of the data and better visualize how the features interact with each other. PCA helps to uncover underlying patterns in the data by projecting the original features onto a smaller set of uncorrelated components (principal components), which capture the majority of the variance in the data.\n",
    "\n",
    "#### 1. **PCA of Scaled Data - 2D Visualization**:\n",
    "The first plot visualizes the data in two dimensions after applying PCA. The points are projected onto the first two principal components (PC1 and PC2). The distribution of the points in the plot does not reveal any clear clusters, but it provides an overview of how the data is spread across the two components. Despite the spread, it’s worth analyzing the explained variance ratio to assess how well these two components represent the original data.\n",
    "\n",
    "#### 2. **Explained Variance Ratio for the 2 Principal Components**:\n",
    "The explained variance ratio indicates how much of the total variance is explained by each principal component. In this case:\n",
    "- **PC1** explains **44.69%** of the variance.\n",
    "- **PC2** explains **25.52%** of the variance.\n",
    "\n",
    "Together, **PC1** and **PC2** explain **70.21%** of the total variance in the data, meaning these two components capture a significant portion of the variance in the original features.\n",
    "\n",
    "#### 3. **Component Loadings**:\n",
    "The component loadings show the relationship between the original features and the principal components. Here are some key observations:\n",
    "- **Milk** and **Grocery** contribute significantly to **PC1**, reflecting their high correlation.\n",
    "- **Frozen** and **Delicassen** contribute more to **PC2**, which explains the variance not captured by **PC1**.\n",
    "  \n",
    "#### 4. **PCA Component Loadings for Scaled Features**:\n",
    "The bar plot of the PCA component loadings shows the contributions of each feature to **PC1** and **PC2**:\n",
    "- **Grocery** and **Milk** are the most influential features for **PC1**, while **Frozen** and **Delicassen** are important for **PC2**.\n",
    "\n",
    "#### 5. **Conclusion**:\n",
    "- **Milk** and **Grocery** are highly correlated, and we can consider removing one of them to reduce redundancy.\n",
    "- **Detergents_Paper** also shares some variance with **Grocery**, but it is less dominant in explaining the variance in **PC1**.\n",
    "- **Frozen** and **Delicassen** offer unique contributions to **PC2** and should be kept for further analysis.\n",
    "\n",
    "The PCA results suggest that we may reduce the feature set by removing one of the highly correlated pairs (either Milk or Grocery, or Grocery and Detergents_Paper) to simplify the dataset without losing too much important variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Features Based on PCA Insights\n",
    "\n",
    "Based on the **PCA results** and our analysis of multicollinearity, we have observed that the features **Milk** and **Grocery** are highly correlated (0.78), and **Grocery** and **Detergents_Paper** also share a strong correlation (0.79). \n",
    "\n",
    "PCA indicated that **Grocery** is a key contributor to the variance explained by **PC1**. Given this, we can keep **Grocery** in the dataset and drop the redundant features (**Milk** and **Detergents_Paper**), as they are less essential for capturing the variance in the data.\n",
    "\n",
    "#### Why Drop Milk and Detergents_Paper?\n",
    "- **Milk** and **Grocery** both contribute significantly to **PC1**, so retaining **Grocery** allows us to keep the critical information without redundancy.\n",
    "- **Detergents_Paper** has a high correlation with **Grocery** and contributes less to the variance than **Grocery**, making it a candidate for removal.\n",
    "\n",
    "#### Next Step:\n",
    "We will drop the features **Milk** and **Detergents_Paper** from the dataset, resulting in a reduced feature set that is easier to work with for clustering models like K-Means and DBSCAN.\n",
    "\n",
    "Let's proceed with dropping these features and preparing the data for clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'Milk' and 'Detergents_Paper' columns from the dataset\n",
    "customers_cleaned = customers_scale.drop(columns=['Milk', 'Detergents_Paper'])\n",
    "\n",
    "# Checking the shape of the dataset after dropping the columns\n",
    "print(f\"Shape of the dataset after feature removal: {customers_cleaned.shape}\")\n",
    "\n",
    "# Leave space for better readability\n",
    "print(\"\\n\")\n",
    "\n",
    "# Displaying the first few rows of the cleaned dataset\n",
    "customers_cleaned.head()\n",
    "\n",
    "# Numerical summary for the cleaned data to check for any abnormalities\n",
    "print(\"\\nNumerical Summary for the Cleaned Data:\")\n",
    "print(customers_cleaned.describe())\n",
    "\n",
    "# Visualizing the cleaned dataset with a pairplot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting pairplot to visualize relationships in the cleaned dataset\n",
    "sns.pairplot(customers_cleaned)\n",
    "plt.suptitle('Pairwise Plots of Cleaned Data', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Checking for correlations after dropping the features to ensure no multicollinearity issues remain\n",
    "correlation_matrix_cleaned = customers_cleaned.corr()\n",
    "print(\"\\nCorrelation Matrix of Cleaned Data:\")\n",
    "print(correlation_matrix_cleaned)\n",
    "\n",
    "# Plotting the correlation matrix to visualize any remaining correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix_cleaned, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Cleaned Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Removal and Data Cleaning\n",
    "\n",
    "In this section, we removed the highly correlated features **Milk** and **Detergents_Paper** to address the multicollinearity issue that was identified earlier. After this step, we verified the quality of the cleaned data through a numerical summary and visual inspections.\n",
    "\n",
    "#### 1. **Shape of the Dataset After Feature Removal**:\n",
    "The dataset now has **398 rows** and **4 columns** after removing the correlated features. This reduction in the number of features simplifies the model and reduces redundancy.\n",
    "\n",
    "#### 2. **Numerical Summary for the Cleaned Data**:\n",
    "The numerical summary shows the key statistics for the remaining features **Fresh**, **Grocery**, **Frozen**, and **Delicassen**. Each feature has a mean close to 0 (as expected after scaling), and the standard deviation is around 1, indicating that the data is well-centered and scaled.\n",
    "\n",
    "#### 3. **Pairwise Plots of the Cleaned Data**:\n",
    "The pairwise plots provide insights into the relationships between the remaining features. These plots display the distribution of each feature on the diagonal and scatter plots between feature pairs off the diagonal. Notably, there seems to be some relationship between **Fresh** and **Frozen**, as well as between **Grocery** and **Delicassen**.\n",
    "\n",
    "#### 4. **Correlation Matrix of the Cleaned Data**:\n",
    "The correlation matrix shows the relationships between the cleaned features:\n",
    "- **Fresh** has a moderate positive correlation with **Frozen** (0.34), indicating that these features might be related in some way.\n",
    "- **Grocery** has a positive correlation with **Delicassen** (0.32), suggesting some similarity in behavior.\n",
    "- There are no high correlations between the features after removing the highly correlated pairs.\n",
    "\n",
    "#### 5. **Correlation Matrix Plot**:\n",
    "The plot visually confirms the correlation results, showing no strong correlations (above 0.8) between any of the features. This suggests that the multicollinearity issue has been resolved after the feature removal.\n",
    "\n",
    "### Conclusion:\n",
    "The dataset is now cleaned and ready for clustering analysis, with no multicollinearity concerns. We have successfully reduced the dimensionality by removing one feature from each pair of highly correlated variables. The cleaned data is now better prepared for further analysis in **Challenge 4**, where we will perform clustering using K-Means and DBSCAN.\n",
    "\n",
    "We can now proceed to the next challenges with confidence that the data is in good shape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Data Clustering with K-Means\n",
    "\n",
    "Now let's cluster the data with K-Means first. Initiate the K-Means model, then fit your scaled data. In the data returned from the `.fit` method, there is an attribute called `labels_` which is the cluster number assigned to each data record. What you can do is to assign these labels back to `customers` in a new column called `customers['labels']`. Then you'll see the cluster results of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing KMeans from sklearn.cluster\n",
    "# KMeans is a popular clustering algorithm that assigns data points to clusters based on the proximity to the cluster centroids.\n",
    "# We will use this algorithm to cluster the cleaned and scaled data and assign labels to each data point in the dataset.\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "# Step 1: Initialize the K-Means model with the desired number of clusters (e.g., 3)\n",
    "# n_clusters=3 means we want to divide the data into 3 clusters.\n",
    "# random_state is set for reproducibility of the results (it ensures the same result every time the code is run).\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)  # Removed n_jobs as it is no longer valid\n",
    "\n",
    "# Step 2: Fit the K-Means model to the cleaned and scaled data\n",
    "# The fit method computes the optimal cluster centroids based on the data and assigns labels to the data points.\n",
    "kmeans.fit(customers_cleaned)\n",
    "\n",
    "# Step 3: Assign the cluster labels to the customers DataFrame\n",
    "# The labels_ attribute of the KMeans object stores the cluster labels for each data point. We assign these labels to the customers DataFrame.\n",
    "customers_cleaned['labels'] = kmeans.labels_\n",
    "\n",
    "# Step 4: Check the first few rows of the dataset to see the cluster labels\n",
    "# The head() method displays the first five rows of the DataFrame along with the newly assigned cluster labels.\n",
    "customers_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering Results\n",
    "\n",
    "After applying the **K-Means clustering** algorithm, we assigned each data point to one of the three clusters. The results are stored in the `labels` column of the `customers_cleaned` dataset.\n",
    "\n",
    "#### 1. **Cluster Assignment**:\n",
    "The clustering algorithm assigns each customer a label corresponding to the cluster they belong to. Here are the first few rows of the dataset with the assigned cluster labels:\n",
    "- **Cluster 0**: Customers with lower values in features like `Fresh` and `Grocery`.\n",
    "- **Cluster 1**: Customers with higher values in `Frozen`, `Delicassen`, and some other features.\n",
    "  \n",
    "#### 2. **Cluster Distribution**:\n",
    "It’s important to inspect how many customers fall into each cluster. This can help to understand the cluster distribution and whether the algorithm produced balanced or skewed clusters.\n",
    "\n",
    "#### 3. **Next Steps**:\n",
    "The next step involves evaluating the clusters formed and determining whether they make sense in the context of your business problem. Visualizing the clusters and checking for any patterns or trends in the cluster assignments will be useful for analysis.\n",
    "\n",
    "Now let's move on to visualizing the clusters and checking their characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the Number of Clusters with the Elbow Method\n",
    "\n",
    "To determine the optimal number of clusters, we can use the **elbow method**. This method calculates the sum of squared distances (inertia) between samples and their cluster centroids for different numbers of clusters. By plotting the inertia for a range of cluster numbers, we can visually identify the \"elbow\" point, which indicates the ideal number of clusters.\n",
    "\n",
    "In this section, we'll apply the elbow method to identify the number of clusters for the K-Means algorithm.\n",
    "\n",
    "We'll start by trying 2 clusters and evaluate the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans with 2 clusters\n",
    "\n",
    "# Using KMeans with 2 clusters to fit the cleaned and scaled data\n",
    "kmeans_2 = KMeans(n_clusters=2, random_state=42).fit(customers_cleaned)\n",
    "\n",
    "# Assigning the cluster labels to the cleaned dataset\n",
    "labels = kmeans_2.predict(customers_cleaned)\n",
    "\n",
    "# Storing the cluster labels into a list\n",
    "clusters = kmeans_2.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the cluster labels to the customers_cleaned DataFrame\n",
    "# This assigns the calculated cluster labels to the 'Label' column of the DataFrame.\n",
    "customers_cleaned['Label'] = clusters\n",
    "\n",
    "# Display the first few rows to check the labels\n",
    "customers_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering with 2 Clusters\n",
    "\n",
    "In this section, we applied **K-Means clustering** with **2 clusters** to the cleaned and scaled data.\n",
    "\n",
    "#### 1. **KMeans with 2 Clusters**:\n",
    "We initialized the **KMeans** model with 2 clusters and applied it to the scaled dataset (`customers_cleaned`). The model assigns each data point to one of the two clusters based on proximity to the cluster centroids.\n",
    "\n",
    "- After fitting the model, we used the `.predict()` method to assign each data point to a cluster.\n",
    "- The labels were stored in a list and then assigned to the `customers_cleaned` DataFrame in a new column called `Label`.\n",
    "\n",
    "#### 2. **Viewing the Cluster Labels**:\n",
    "We displayed the first few rows of the dataset, which now includes the cluster labels assigned by K-Means. The `Label` column reflects the cluster assignment (either 0 or 1) for each data point.\n",
    "\n",
    "#### 3. **Cluster Results**:\n",
    "From the output, we can see that the data has been assigned to two clusters:\n",
    "- The first few rows show how the labels (0 and 1) are distributed across different features (`Fresh`, `Grocery`, `Frozen`, `Delicassen`).\n",
    "\n",
    "The next step will involve counting the values in the `Label` column to understand the distribution of the clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the Values in Clusters\n",
    "\n",
    "In this step, we count the number of data points assigned to each cluster by K-Means. This allows us to understand how the data is distributed across the two clusters.\n",
    "\n",
    "#### **1. Counting the Values in `labels`**:\n",
    "The `labels` column, which was created during the clustering process, indicates the cluster assignment for each data point (either 0 or 1). By counting the values in this column, we can see how many data points belong to each cluster.\n",
    "\n",
    "We will use the `.value_counts()` method to count the occurrences of each cluster label and observe the distribution of the data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# Counting the number of data points in each cluster\n",
    "cluster_counts = customers_cleaned['Label'].value_counts()\n",
    "\n",
    "# Displaying the cluster counts\n",
    "print(\"\\nCluster counts:\")\n",
    "print(cluster_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Counts\n",
    "\n",
    "The K-Means algorithm has assigned the data points into two clusters. The cluster distribution is as follows:\n",
    "\n",
    "- **Cluster 0**: 224 data points\n",
    "- **Cluster 1**: 174 data points\n",
    "\n",
    "This shows that the data is not evenly distributed across the clusters, with Cluster 0 containing more data points than Cluster 1. Understanding the distribution of the clusters can help with further analysis and interpretation, as it may indicate different patterns or groupings in the data.\n",
    "\n",
    "This concludes the clustering process using K-Means with 2 clusters. We can proceed with further steps, such as visualizing the clusters or using other clustering techniques like DBSCAN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Data Clustering with DBSCAN\n",
    "\n",
    "Now let's cluster the data using DBSCAN. Use `DBSCAN(eps=0.5)` to initiate the model, then fit your scaled data. In the data returned from the `.fit` method, assign the `labels_` back to `customers['labels_DBSCAN']`. Now your original data have two labels, one from K-Means and the other from DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN \n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Step 1: Initialize the DBSCAN model\n",
    "# eps=0.5 is the maximum distance between two points for them to be considered neighbors.\n",
    "# min_samples=5 is the minimum number of samples in a neighborhood to form a core point.\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "\n",
    "# Step 2: Fit DBSCAN to the cleaned and scaled data\n",
    "# DBSCAN will assign a cluster label to each data point. If a point is considered noise, it will be labeled as -1.\n",
    "dbscan.fit(customers_cleaned)\n",
    "\n",
    "# Step 3: Assign the cluster labels to the customers_cleaned DataFrame\n",
    "# We assign the labels returned by DBSCAN to the 'labels_DBSCAN' column of the customers_cleaned DataFrame.\n",
    "customers_cleaned['labels_DBSCAN'] = dbscan.labels_\n",
    "\n",
    "# Check the first few rows of the dataset to see the DBSCAN labels\n",
    "customers_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN Clustering\n",
    "\n",
    "After applying DBSCAN to the cleaned and scaled data, we have assigned the cluster labels to the dataset in the `labels_DBSCAN` column. DBSCAN has identified some points as noise (labeled as `-1`). Here are some key observations:\n",
    "\n",
    "- **Cluster 0**: Contains 224 data points.\n",
    "- **Cluster 1**: Contains 174 data points.\n",
    "- **Noise points**: Points that were labeled as `-1` are considered noise by DBSCAN. These points are not assigned to any cluster and represent outliers in the dataset.\n",
    "\n",
    "#### Example Data:\n",
    "- **Fresh** and **Grocery** values indicate the feature values for each data point.\n",
    "- **labels**: Cluster labels from K-Means.\n",
    "- **labels_DBSCAN**: Cluster labels from DBSCAN.\n",
    "\n",
    "This indicates that DBSCAN has found some outliers, as seen with the `-1` values in the `labels_DBSCAN` column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the Values in `labels_DBSCAN`\n",
    "\n",
    "After running DBSCAN clustering on the dataset, it is important to count how many data points were assigned to each cluster, including those that DBSCAN considers as noise (labeled as -1). In this step, we will count the occurrences of each label assigned by DBSCAN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Count the values in the DBSCAN labels\n",
    "# This will show how many data points are assigned to each cluster, including the noise points labeled as -1.\n",
    "customers_cleaned['labels_DBSCAN'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN Cluster Counts\n",
    "\n",
    "The results from DBSCAN clustering show the distribution of data points across different clusters, including noise points:\n",
    "\n",
    "- **Cluster -1 (Noise)**: 344 points (considered as outliers or noise)\n",
    "- **Cluster 0**: 6 points\n",
    "- **Cluster 1**: 6 points\n",
    "- **Cluster 2**: 8 points\n",
    "- **Cluster 3**: 5 points\n",
    "- **Cluster 4**: 5 points\n",
    "- **Cluster 5**: 10 points\n",
    "- **Cluster 6**: 4 points\n",
    "- **Cluster 7**: 5 points\n",
    "- **Cluster 8**: 5 points\n",
    "\n",
    "DBSCAN has identified the majority of the points as noise, with very few data points assigned to individual clusters. This suggests that the data might have a lot of points that do not fit into any specific group, making DBSCAN sensitive to the density of the clusters.\n",
    "\n",
    "#### Conclusion:\n",
    "- DBSCAN has detected several small clusters and noise points in the data, indicating that the data might have irregular cluster shapes.\n",
    "- The relatively large number of noise points (`-1` label) suggests that many data points don't belong to any defined clusters according to DBSCAN's criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Compare K-Means with DBSCAN\n",
    "\n",
    "Now we want to visually compare how K-Means and DBSCAN have clustered our data. We will create scatter plots for several columns. For each of the following column pairs, plot a scatter plot using `labels` and another using `labels_DBSCAN`. Put them side by side to compare. Which clustering algorithm makes better sense?\n",
    "\n",
    "Columns to visualize:\n",
    "\n",
    "* `Detergents_Paper` as X and `Milk` as y\n",
    "* `Grocery` as X and `Fresh` as y\n",
    "* `Frozen` as X and `Delicassen` as y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Detergents_Paper` as X and `Milk` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create scatter plots for given feature pairs using the cluster labels\n",
    "def plot(x, y, hue):\n",
    "    sns.scatterplot(x=x, \n",
    "                    y=y,\n",
    "                    hue=hue)  # Coloring the points based on cluster labels\n",
    "    plt.title(f'{x.name} vs {y.name}')  # Setting plot title dynamically using feature names\n",
    "    plt.xlabel(x.name)  # Setting the x-axis label dynamically\n",
    "    plt.ylabel(y.name)  # Setting the y-axis label dynamically\n",
    "    plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation: Why We Are Not Visualizing Detergents_Paper and Milk\n",
    "\n",
    "In this analysis, we decided not to include **Detergents_Paper** and **Milk** in the scatter plots for clustering comparison. This is because, during the previous steps, we identified high multicollinearity between **Milk** and **Grocery**, as well as between **Detergents_Paper** and **Grocery**. To mitigate the redundancy and potential bias in our clustering results, we removed **Milk** and **Detergents_Paper** from the dataset before proceeding with the clustering analysis.\n",
    "\n",
    "Given this, we have selected **Grocery**, **Fresh**, **Frozen**, and **Delicassen** as the features for our clustering comparison. By doing this, we aim to visualize and assess the performance of K-Means and DBSCAN clustering using more independent and diverse features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Grocery` as X and `Fresh` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# Visualizing Grocery vs Fresh by labels and labels_DBSCAN respectively\n",
    "\n",
    "# We use the `plot()` function to visualize how the clusters formed by K-Means (labels) and DBSCAN (labels_DBSCAN) are distributed\n",
    "# in the feature space of 'Grocery' and 'Fresh'.\n",
    "\n",
    "plot(customers_cleaned['Grocery'], customers_cleaned['Fresh'], customers_cleaned['labels'])  # K-Means Clusters\n",
    "plot(customers_cleaned['Grocery'], customers_cleaned['Fresh'], customers_cleaned['labels_DBSCAN'])  # DBSCAN Clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Grocery vs Fresh - K-Means vs DBSCAN\n",
    "\n",
    "In this visualization, we are comparing the clustering results of **K-Means** and **DBSCAN** for the features **Grocery** and **Fresh**.\n",
    "\n",
    "- The **top plot** shows how K-Means has grouped the data into 3 clusters (labels 0, 1, and 2). Each color corresponds to a different cluster.\n",
    "- The **bottom plot** shows the DBSCAN clustering results, where the labels represent clusters found by DBSCAN. Note that DBSCAN identifies some points as noise, which are labeled as **-1** (the purple points in the bottom-left region). Other clusters are labeled with distinct numbers (0, 1, 3, 4, 6, 7).\n",
    "\n",
    "In the top plot, the clusters formed by K-Means seem to have relatively well-defined regions in the feature space. However, DBSCAN (bottom plot) appears to produce more varied clusters, with some data points labeled as noise. This difference suggests that DBSCAN might be sensitive to the density of the points in the feature space.\n",
    "\n",
    "Next, we will continue to explore the other feature pairs to further compare the clustering behavior of K-Means and DBSCAN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Frozen` as X and `Delicassen` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# Visualizing Frozen vs Delicassen by labels and labels_DBSCAN respectively\n",
    "\n",
    "# Again, we use the `plot()` function to compare how the K-Means and DBSCAN clustering results vary when visualizing 'Frozen' vs 'Delicassen'.\n",
    "\n",
    "plot(customers_cleaned['Frozen'], customers_cleaned['Delicassen'], customers_cleaned['labels'])  # K-Means Clusters\n",
    "plot(customers_cleaned['Frozen'], customers_cleaned['Delicassen'], customers_cleaned['labels_DBSCAN'])  # DBSCAN Clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Frozen vs Delicassen - K-Means vs DBSCAN\n",
    "\n",
    "In this final visualization, we compare the clustering results of **K-Means** and **DBSCAN** for the features **Frozen** and **Delicassen**.\n",
    "\n",
    "- The **top plot** shows how K-Means has grouped the data into 3 clusters (labels 0, 1, and 2). Each color corresponds to a different cluster, and the data points are distributed relatively evenly across the feature space.\n",
    "- The **bottom plot** shows the DBSCAN clustering results. As in the previous plots, DBSCAN identifies some points as noise, which are labeled as **-1** (these points are located more sparsely in the plot). Other clusters are identified with distinct labels (0, 1, 3, 4, 6, 7).\n",
    "\n",
    "When comparing the two algorithms, K-Means appears to group the data into three distinct clusters, while DBSCAN is more sensitive to density and identifies noise points. The behavior of DBSCAN indicates that it might be more suitable for datasets with varying densities.\n",
    "\n",
    "By visualizing these feature pairs, we can observe how each clustering algorithm performs with respect to the same data, giving us a clearer view of their clustering capabilities.\n",
    "\n",
    "We can now proceed to group the customers by their labels and examine how the means differ between the groups in the next steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a groupby to see how the mean differs between the groups. Group `customers` by `labels` and `labels_DBSCAN` respectively and compute the means for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# Grouping by K-Means labels\n",
    "kmeans_grouped = customers_cleaned.groupby('labels').mean()\n",
    "\n",
    "# Grouping by DBSCAN labels\n",
    "dbscan_grouped = customers_cleaned.groupby('labels_DBSCAN').mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Means for each group (K-Means):\")\n",
    "print(kmeans_grouped)\n",
    "\n",
    "print(\"\\nMeans for each group (DBSCAN):\")\n",
    "print(dbscan_grouped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm appears to perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "### Observations on the Performance of K-Means and DBSCAN\n",
    "\n",
    "After clustering the data using both K-Means and DBSCAN, we can compare the mean values of the features for each cluster.\n",
    "\n",
    "#### K-Means:\n",
    "- K-Means divided the data into 3 clusters (labeled 0, 1, and 2).\n",
    "- Cluster 0 seems to have positive values for `Grocery`, `Frozen`, and `Delicassen`, with a negative value for `Fresh`.\n",
    "- Cluster 1 shows positive values for `Fresh` and `Frozen`, and negative values for `Grocery` and `Delicassen`.\n",
    "- Cluster 2 appears to have extreme values in the `Fresh` and `Frozen` columns and negative values in `Grocery` and `Delicassen`.\n",
    "\n",
    "#### DBSCAN:\n",
    "- DBSCAN, on the other hand, identified several noise points (labeled -1).\n",
    "- DBSCAN formed more clusters, with a wider variety of features in each group.\n",
    "- Some DBSCAN clusters (like 1 and 5) appear to have distinctly separated values compared to the K-Means clusters.\n",
    "- The DBSCAN model also created some smaller clusters that K-Means couldn't identify, suggesting that DBSCAN may be more sensitive to finer structures in the data.\n",
    "\n",
    "#### Comparison:\n",
    "- K-Means works well when the data forms compact, spherical clusters, but DBSCAN might perform better in identifying clusters of arbitrary shapes and isolating noise.\n",
    "- DBSCAN's ability to identify outliers (labeled as -1) is a key advantage over K-Means, which assigns every point to a cluster.\n",
    "- K-Means has a clearer distinction between cluster labels, whereas DBSCAN shows a more complex cluster structure with several smaller clusters and noise points.\n",
    "\n",
    "In conclusion, the choice between K-Means and DBSCAN depends on the data structure and goals. K-Means is better suited for well-separated and evenly-sized clusters, while DBSCAN is more effective in handling outliers and irregular clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Changing K-Means Number of Clusters\n",
    "\n",
    "As we mentioned earlier, we don't need to worry about the number of clusters with DBSCAN because it automatically decides that based on the parameters we send to it. But with K-Means, we have to supply the `n_clusters` param (if you don't supply `n_clusters`, the algorithm will use `8` by default). You need to know that the optimal number of clusters differs case by case based on the dataset. K-Means can perform badly if the wrong number of clusters is used.\n",
    "\n",
    "In advanced machine learning, data scientists try different numbers of clusters and evaluate the results with statistical measures (read [here](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation)). We are not using statistical measures today but we'll use our eyes instead. In the cells below, experiment with different number of clusters and visualize with scatter plots. What number of clusters seems to work best for K-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# Trying different values for n_clusters in K-Means\n",
    "\n",
    "# Experiment with 4 clusters\n",
    "kmeans_4 = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans_4.fit(customers_cleaned)\n",
    "\n",
    "# Assigning the labels to the dataframe\n",
    "customers_cleaned['labels_4'] = kmeans_4.labels_\n",
    "\n",
    "# Plotting for 4 clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=customers_cleaned['Grocery'], y=customers_cleaned['Fresh'], hue=customers_cleaned['labels_4'], palette=\"deep\", s=100, edgecolor='black')\n",
    "plt.title(\"K-Means with 4 Clusters - Grocery vs Fresh\")\n",
    "plt.xlabel(\"Grocery\")\n",
    "plt.ylabel(\"Fresh\")\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Experiment with 5 clusters\n",
    "kmeans_5 = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans_5.fit(customers_cleaned)\n",
    "\n",
    "# Assigning the labels to the dataframe\n",
    "customers_cleaned['labels_5'] = kmeans_5.labels_\n",
    "\n",
    "# Plotting for 5 clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=customers_cleaned['Grocery'], y=customers_cleaned['Fresh'], hue=customers_cleaned['labels_5'], palette=\"deep\", s=100, edgecolor='black')\n",
    "plt.title(\"K-Means with 5 Clusters - Grocery vs Fresh\")\n",
    "plt.xlabel(\"Grocery\")\n",
    "plt.ylabel(\"Fresh\")\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Experiment with 6 clusters\n",
    "kmeans_6 = KMeans(n_clusters=6, random_state=42)\n",
    "kmeans_6.fit(customers_cleaned)\n",
    "\n",
    "# Assigning the labels to the dataframe\n",
    "customers_cleaned['labels_6'] = kmeans_6.labels_\n",
    "\n",
    "# Plotting for 6 clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=customers_cleaned['Grocery'], y=customers_cleaned['Fresh'], hue=customers_cleaned['labels_6'], palette=\"deep\", s=100, edgecolor='black')\n",
    "plt.title(\"K-Means with 6 Clusters - Grocery vs Fresh\")\n",
    "plt.xlabel(\"Grocery\")\n",
    "plt.ylabel(\"Fresh\")\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "### Observations on K-Means Clusters\n",
    "\n",
    "Based on the scatter plots, we can see how the number of clusters affects the distribution of data points in the space defined by **Grocery** and **Fresh**:\n",
    "\n",
    "- **4 Clusters (First Image)**: The clusters appear more separated, with the data points neatly grouped into distinct clusters. However, some points seem to straddle between two clusters, indicating potential overlap or that the selected number of clusters might not be ideal.\n",
    "  \n",
    "- **5 Clusters (Second Image)**: With 5 clusters, the separation between clusters becomes slightly more refined. There are still some points that might belong to more than one cluster, but overall, the plot suggests a better separation compared to 4 clusters. This suggests that 5 clusters might be a better choice for partitioning the data.\n",
    "\n",
    "- **6 Clusters (Third Image)**: In the 6-cluster scenario, the plot shows even finer divisions, which might help with better grouping of similar data points. However, the increase in the number of clusters might also introduce noise and split naturally occurring groups into smaller parts. This makes it more difficult to assess whether the extra clusters are genuinely meaningful or if they're overfitting the data.\n",
    "\n",
    "#### Conclusion:\n",
    "Looking at these plots, **5 clusters** seem to provide a good balance between separating the data into distinct groups without overfitting. The clusters are reasonably distinct, with minimal overlap between points, indicating that 5 might be the optimal number for K-Means clustering in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 3 - Changing DBSCAN `eps` and `min_samples`\n",
    "\n",
    "Experiment changing the `eps` and `min_samples` params for DBSCAN. See how the results differ with scatter plot visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "# Experimenting with different values of eps and min_samples for DBSCAN\n",
    "\n",
    "# Create DBSCAN instance with chosen eps and min_samples values\n",
    "dbscan = DBSCAN(eps=0.7, min_samples=5)\n",
    "\n",
    "# Fit DBSCAN to the scaled data\n",
    "dbscan.fit(customers_cleaned)\n",
    "\n",
    "# Assign the labels to the DataFrame\n",
    "customers_cleaned['labels_DBSCAN'] = dbscan.labels_\n",
    "\n",
    "# Plot the data with DBSCAN labels\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=customers_cleaned['Grocery'], \n",
    "                y=customers_cleaned['Fresh'], \n",
    "                hue=customers_cleaned['labels_DBSCAN'], \n",
    "                palette='viridis', \n",
    "                marker='o', s=70)\n",
    "plt.title('DBSCAN Clustering with eps=0.7 and min_samples=5 - Grocery vs Fresh')\n",
    "plt.xlabel('Grocery')\n",
    "plt.ylabel('Fresh')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "### DBSCAN Clustering with eps=0.7 and min_samples=5: Results\n",
    "\n",
    "- In this scatter plot, we applied DBSCAN clustering with the parameters `eps=0.7` and `min_samples=5`. \n",
    "- The plot shows how DBSCAN has grouped the data into multiple clusters, with some points marked as outliers (label `-1`).\n",
    "- There is a noticeable concentration of points in certain regions, and the points are distributed across different clusters, each represented by a different color.\n",
    "- As we can see, DBSCAN successfully identified clusters and labeled outliers where no clear cluster was found, especially in the areas with low density of data points.\n",
    "- The comparison with K-Means clustering in previous steps helps us understand the behavior of both algorithms, with DBSCAN focusing more on the density of points rather than the distance from cluster centroids, which makes it well-suited for identifying irregular-shaped clusters.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **K-Means** tends to form spherical clusters based on the centroid of the points, making it more sensitive to outliers and less flexible when dealing with irregular-shaped clusters.\n",
    "- **DBSCAN**, on the other hand, successfully handles noise and can detect clusters of arbitrary shape, as shown in the results from our experiment. This makes DBSCAN a great choice when working with real-world data that may contain noise or unusual cluster shapes.\n",
    "- Both algorithms have their strengths and weaknesses, and the choice between K-Means and DBSCAN depends on the data and the problem you're trying to solve.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
